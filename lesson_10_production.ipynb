{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10: Production-Ready Agents\n",
    "\n",
    "This interactive notebook teaches you how to build production-ready agents:\n",
    "\n",
    "- ‚úÖ Guardrails for content filtering and safety\n",
    "- ‚úÖ PII redaction with hooks for privacy protection\n",
    "- ‚úÖ OpenTelemetry for metrics and traces\n",
    "- ‚úÖ Agent performance monitoring and evaluation\n",
    "- ‚úÖ Production deployment best practices\n",
    "\n",
    "**Estimated time:** 6-7 hours\n",
    "\n",
    "**What you'll build:** Production-ready agents with safety, observability, and quality assurance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary modules and configure the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lesson_utils import load_environment, create_working_model, check_api_keys\n",
    "from strands import Agent\n",
    "from strands.hooks import HookProvider, HookRegistry, MessageAddedEvent, AfterInvocationEvent\n",
    "\n",
    "# Load environment and check API keys\n",
    "load_environment()\n",
    "check_api_keys()\n",
    "\n",
    "print(\"üéØ Lesson 10: Production-Ready Agents\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Safety & Security - Guardrails and PII Redaction\n",
    "\n",
    "Production agents require safety mechanisms:\n",
    "\n",
    "### Guardrails\n",
    "- **Content filtering** - Block harmful content\n",
    "- **Topic boundaries** - Enforce allowed domains\n",
    "- **PII protection** - Detect and redact sensitive data\n",
    "- **Quality enforcement** - Maintain response standards\n",
    "\n",
    "### Implementation Patterns:\n",
    "1. **Bedrock Guardrails** - Built-in AWS protection\n",
    "2. **Notify-Only Mode** - Shadow testing with hooks\n",
    "3. **PII Redaction** - Third-party libraries (LLM Guard, Presidio)\n",
    "\n",
    "**Reference:** [Strands Guardrails](https://strandsagents.com/latest/documentation/docs/user-guide/safety-security/guardrails/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_working_model()\n",
    "\n",
    "if model:\n",
    "    print(\"\\nüìã Implementing Notify-Only Guardrails with Hooks...\\n\")\n",
    "\n",
    "    class NotifyOnlyGuardrailsHook(HookProvider):\n",
    "        \"\"\"Hook-based guardrails that notify without blocking.\"\"\"\n",
    "        def __init__(self):\n",
    "            self.blocked_patterns = [\"sensitive\", \"confidential\", \"secret\", \"password\"]\n",
    "\n",
    "        def register_hooks(self, registry: HookRegistry) -> None:\n",
    "            registry.add_callback(MessageAddedEvent, self.check_user_input)\n",
    "            registry.add_callback(AfterInvocationEvent, self.check_assistant_response)\n",
    "\n",
    "        def evaluate_content(self, content: str, source: str = \"INPUT\"):\n",
    "            content_lower = content.lower()\n",
    "            violations = [p for p in self.blocked_patterns if p in content_lower]\n",
    "            if violations:\n",
    "                print(f\"\\n[GUARDRAIL] WOULD BLOCK - {source}: {content[:100]}...\")\n",
    "                print(f\"[GUARDRAIL] Violations: {', '.join(violations)}\\n\")\n",
    "\n",
    "        def check_user_input(self, event: MessageAddedEvent) -> None:\n",
    "            if event.message.get(\"role\") == \"user\":\n",
    "                content = \"\".join(block.get(\"text\", \"\") for block in event.message.get(\"content\", []))\n",
    "                if content:\n",
    "                    self.evaluate_content(content, \"INPUT\")\n",
    "\n",
    "        def check_assistant_response(self, event: AfterInvocationEvent) -> None:\n",
    "            if event.agent.messages and event.agent.messages[-1].get(\"role\") == \"assistant\":\n",
    "                content = \"\".join(block.get(\"text\", \"\") for block in event.agent.messages[-1].get(\"content\", []))\n",
    "                if content:\n",
    "                    self.evaluate_content(content, \"OUTPUT\")\n",
    "\n",
    "    # Create agent with guardrail monitoring\n",
    "    agent = Agent(\n",
    "        model=model,\n",
    "        system_prompt=\"You are a helpful assistant.\",\n",
    "        hooks=[NotifyOnlyGuardrailsHook()]\n",
    "    )\n",
    "\n",
    "    print(\"‚úì Agent created with notify-only guardrails\\n\")\n",
    "    \n",
    "    # Test with safe and potentially sensitive content\n",
    "    response = agent(\"What is machine learning? Answer in 2 sentences.\")\n",
    "    print(f\"Response: {response}\\n\")\n",
    "    \n",
    "    response = agent(\"Tell me about sensitive data handling in 2 sentences\")\n",
    "    print(f\"Response: {str(response)[:150]}...\")\n",
    "\n",
    "    print(\"\\nüí° Key Takeaway: Hooks enable flexible safety monitoring without blocking content\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No API key available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Observability - Metrics and Traces\n",
    "\n",
    "**Observability** enables monitoring agent behavior and performance.\n",
    "\n",
    "### Key Metrics:\n",
    "- **Token Usage** - Input, output, total tokens (cost optimization)\n",
    "- **Performance** - Latency, cycle count, execution time\n",
    "- **Tool Usage** - Call counts, success rates, execution times\n",
    "- **Traces** - Complete execution flow with OpenTelemetry\n",
    "\n",
    "### Built-in Metrics:\n",
    "Every agent invocation returns an `AgentResult` with comprehensive metrics.\n",
    "\n",
    "**Reference:** [Strands Observability](https://strandsagents.com/latest/documentation/docs/user-guide/observability-evaluation/observability/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_working_model()\n",
    "\n",
    "if model:\n",
    "    print(\"\\nüìä Capturing and Analyzing Agent Metrics...\\n\")\n",
    "    \n",
    "    agent = Agent(\n",
    "        model=model,\n",
    "        system_prompt=\"You are a helpful assistant that provides concise responses.\"\n",
    "    )\n",
    "\n",
    "    # Invoke agent and capture metrics\n",
    "    result = agent(\"Explain what an AI agent is in 2 sentences\")\n",
    "    print(f\"Response: {result}\\n\")\n",
    "\n",
    "    # Access metrics\n",
    "    print(\"üìà Metrics Analysis:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    usage = result.metrics.accumulated_usage\n",
    "    print(f\"\\nüí∞ Token Usage:\")\n",
    "    print(f\"   Input tokens:  {usage['inputTokens']}\")\n",
    "    print(f\"   Output tokens: {usage['outputTokens']}\")\n",
    "    print(f\"   Total tokens:  {usage['totalTokens']}\")\n",
    "\n",
    "    metrics_data = result.metrics.accumulated_metrics\n",
    "    print(f\"\\n‚ö° Performance:\")\n",
    "    print(f\"   Latency: {metrics_data['latencyMs']}ms\")\n",
    "    print(f\"   Cycles:  {result.metrics.cycle_count}\")\n",
    "\n",
    "    if result.metrics.cycle_durations:\n",
    "        avg_cycle = sum(result.metrics.cycle_durations) / len(result.metrics.cycle_durations)\n",
    "        print(f\"   Avg cycle time: {avg_cycle:.3f}s\")\n",
    "\n",
    "    # Comprehensive summary\n",
    "    print(\"\\nüìã Comprehensive Summary:\")\n",
    "    summary = result.metrics.get_summary()\n",
    "    print(f\"   Total Duration: {summary.get('total_duration', 0):.3f}s\")\n",
    "    print(f\"   Total Cycles: {summary.get('total_cycles', 0)}\")\n",
    "    print(f\"   Avg Cycle Time: {summary.get('average_cycle_time', 0):.3f}s\")\n",
    "\n",
    "    print(\"\\nüí° Use these metrics to optimize performance and control costs!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No API key available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenTelemetry Integration\n",
    "\n",
    "For production monitoring, integrate with OpenTelemetry:\n",
    "\n",
    "```python\n",
    "# Installation\n",
    "pip install 'strands-agents[otel]'\n",
    "\n",
    "# Setup\n",
    "from strands.telemetry import StrandsTelemetry\n",
    "\n",
    "telemetry = StrandsTelemetry()\n",
    "telemetry.setup_otlp_exporter()      # Send to collector\n",
    "telemetry.setup_console_exporter()   # Print to console\n",
    "\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    trace_attributes={\n",
    "        'session.id': 'abc-1234',\n",
    "        'user.id': 'user@example.com',\n",
    "        'environment': 'production'\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**Integration platforms:**\n",
    "- Jaeger - Open-source tracing\n",
    "- AWS X-Ray - AWS native\n",
    "- Datadog, New Relic - Commercial\n",
    "- Langfuse - AI-specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation - Testing and Quality Assurance\n",
    "\n",
    "**Agent Evaluation** ensures consistent quality and performance.\n",
    "\n",
    "### Evaluation Patterns:\n",
    "\n",
    "1. **Metrics-Based Testing**\n",
    "   - Latency < SLA threshold\n",
    "   - Token usage within budget\n",
    "   - Tool success rate > 95%\n",
    "\n",
    "2. **Functional Testing**\n",
    "   - Core capabilities work\n",
    "   - Tools execute correctly\n",
    "   - Error handling works\n",
    "\n",
    "3. **Quality Testing**\n",
    "   - Response relevance\n",
    "   - Tone consistency\n",
    "   - Safety compliance\n",
    "\n",
    "4. **Regression Testing**\n",
    "   - Previous bugs don't return\n",
    "   - Quality maintained\n",
    "   - Performance stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_working_model()\n",
    "\n",
    "if model:\n",
    "    print(\"\\nüìä Running Performance Evaluation...\\n\")\n",
    "\n",
    "    agent = Agent(\n",
    "        model=model,\n",
    "        system_prompt=\"You are a helpful assistant.\"\n",
    "    )\n",
    "\n",
    "    # Define test cases with performance criteria\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"query\": \"What is 2+2?\",\n",
    "            \"max_latency_ms\": 5000,\n",
    "            \"max_tokens\": 100,\n",
    "            \"description\": \"Simple arithmetic\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Explain quantum computing in one sentence\",\n",
    "            \"max_latency_ms\": 8000,\n",
    "            \"max_tokens\": 150,\n",
    "            \"description\": \"Concise explanation\"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"Test {i}/{len(test_cases)}: {test_case['description']}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        result = agent(test_case[\"query\"])\n",
    "\n",
    "        # Evaluate metrics\n",
    "        latency = result.metrics.accumulated_metrics[\"latencyMs\"]\n",
    "        total_tokens = result.metrics.accumulated_usage[\"totalTokens\"]\n",
    "\n",
    "        passed = True\n",
    "        issues = []\n",
    "\n",
    "        if latency > test_case[\"max_latency_ms\"]:\n",
    "            passed = False\n",
    "            issues.append(f\"Latency {latency}ms > {test_case['max_latency_ms']}ms\")\n",
    "\n",
    "        if total_tokens > test_case[\"max_tokens\"]:\n",
    "            passed = False\n",
    "            issues.append(f\"Tokens {total_tokens} > {test_case['max_tokens']}\")\n",
    "\n",
    "        status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
    "        print(f\"Status: {status}\")\n",
    "        print(f\"Latency: {latency}ms (max: {test_case['max_latency_ms']}ms)\")\n",
    "        print(f\"Tokens: {total_tokens} (max: {test_case['max_tokens']})\")\n",
    "\n",
    "        if issues:\n",
    "            print(f\"Issues: {', '.join(issues)}\")\n",
    "\n",
    "        results.append({\"test\": test_case[\"description\"], \"passed\": passed})\n",
    "        print()\n",
    "\n",
    "    # Summary\n",
    "    passed_count = sum(1 for r in results if r[\"passed\"])\n",
    "    total_count = len(results)\n",
    "    pass_rate = (passed_count / total_count) * 100 if total_count > 0 else 0\n",
    "\n",
    "    print(\"üìà Evaluation Summary:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Tests Run: {total_count}\")\n",
    "    print(f\"Passed: {passed_count}\")\n",
    "    print(f\"Failed: {total_count - passed_count}\")\n",
    "    print(f\"Pass Rate: {pass_rate:.1f}%\")\n",
    "\n",
    "    print(\"\\nüí° Regular evaluation ensures production quality!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No API key available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Production Deployment Best Practices\n",
    "\n",
    "### üõ°Ô∏è Security and Privacy:\n",
    "- ‚úì Enable guardrails for content safety\n",
    "- ‚úì Implement PII redaction\n",
    "- ‚úì Use secure credential management\n",
    "- ‚úì Encrypt data in transit and at rest\n",
    "- ‚úì Regular security audits\n",
    "\n",
    "### üìä Monitoring and Alerting:\n",
    "- ‚úì Set up OpenTelemetry tracing\n",
    "- ‚úì Monitor token usage and costs\n",
    "- ‚úì Track latency and error rates\n",
    "- ‚úì Alert on anomalies\n",
    "- ‚úì Dashboard for key metrics\n",
    "\n",
    "### ‚ö° Scalability and Performance:\n",
    "- ‚úì Async/streaming for responsiveness\n",
    "- ‚úì Caching for repeated queries\n",
    "- ‚úì Connection pooling\n",
    "- ‚úì Load balancing\n",
    "- ‚úì Auto-scaling\n",
    "\n",
    "### üí∞ Cost Optimization:\n",
    "- ‚úì Monitor token usage patterns\n",
    "- ‚úì Use caching to reduce API calls\n",
    "- ‚úì Optimize prompts\n",
    "- ‚úì Choose appropriate models\n",
    "- ‚úì Set budget alerts\n",
    "\n",
    "### üö® Incident Response:\n",
    "- ‚úì Defined escalation procedures\n",
    "- ‚úì Runbooks for common issues\n",
    "- ‚úì Fallback mechanisms\n",
    "- ‚úì Circuit breakers\n",
    "- ‚úì Post-incident reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Exercises:\n",
    "1. **Integrate AWS Bedrock Guardrails** - Add real guardrails with AWS\n",
    "2. **Implement PII Redaction** - Use LLM Guard or Presidio\n",
    "3. **Set up OpenTelemetry** - Connect to Jaeger locally\n",
    "4. **Create Test Suite** - Build 20+ structured test cases\n",
    "5. **LLM Judge** - Implement automated quality evaluation\n",
    "6. **Metrics Dashboard** - Visualize performance data\n",
    "7. **Alerting** - Set up latency/error thresholds\n",
    "8. **Caching Layer** - Reduce costs with caching\n",
    "9. **Incident Runbook** - Document response procedures\n",
    "10. **Canary Deployment** - Implement safe rollout strategy\n",
    "\n",
    "Use the cell below for your experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Success Criteria\n",
    "\n",
    "You've completed Lesson 10 if:\n",
    "\n",
    "- ‚úÖ Understand guardrails for content safety\n",
    "- ‚úÖ Can implement PII redaction with hooks\n",
    "- ‚úÖ Access and interpret agent metrics\n",
    "- ‚úÖ Understand OpenTelemetry integration\n",
    "- ‚úÖ Can design structured test suites\n",
    "- ‚úÖ Evaluate agents with quantitative metrics\n",
    "- ‚úÖ Know production deployment best practices\n",
    "- ‚úÖ Understand security, monitoring, and scalability\n",
    "\n",
    "## üí° Key Concepts Learned\n",
    "\n",
    "- **Guardrails** - Content filtering, topic blocking, PII protection\n",
    "- **Hooks** - Flexible integration points for safety/monitoring\n",
    "- **Observability** - Metrics, traces, logs for production monitoring\n",
    "- **OpenTelemetry** - Industry standard for distributed tracing\n",
    "- **Evaluation** - Systematic testing and quality assurance\n",
    "- **Best Practices** - Security, monitoring, scalability, cost optimization\n",
    "\n",
    "## üéì Congratulations!\n",
    "\n",
    "You've completed all 10 lessons of the Strands Agent Framework!\n",
    "\n",
    "### What You've Mastered:\n",
    "- Foundation patterns (basic agents, tools)\n",
    "- Intermediate patterns (state, async, multi-modal)\n",
    "- Advanced patterns (hooks, context, conversation management)\n",
    "- Multi-agent patterns (Graph, Swarm, Workflow, A2A)\n",
    "- Production patterns (safety, observability, evaluation)\n",
    "\n",
    "### Next Steps:\n",
    "- Build your own production agent\n",
    "- Explore AWS deployment options\n",
    "- Contribute to Strands community\n",
    "- Share your projects!\n",
    "\n",
    "**You're now ready to build production-ready AI agents!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Strands)",
   "language": "python",
   "name": "strands-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
